# -*- coding: utf-8 -*-
"""
Created on Wed Mar 21 10:46:46 2018

@author: yifei
"""

import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
import numpy as np


def train_classifier(x_train, y_train, x_test, y_test,
                     batch_size=32, epochs=25,
                     lr=0.0001, decay=1e-6,  # for optimizer
                     data_augmentation=True):
    """
        Train a cifar10 classifier using the official keras example
        :param model:
        :param x_train: training data, size (N x h x w x 3)
        :param y_train: categorical ground truth for training data, size (N x C)
        :param x_test: testing data, size (N x h x w x 3)
        :param y_test: categorical ground truth for testing data, size (N x C)
        :param batch_size: batch size for training
        :param epochs: epochs for training
        :param lr: learning rate
        :param decay: decay of learning rate
        :param data_augmentation: whether using data augmentation or not
        :return: the model after training
    """

    num_classes = y_train.shape[1]

    model = Sequential()
    model.add(Conv2D(32, (3, 3), padding='same',
                     input_shape=x_train.shape[1:]))
    model.add(Activation('relu'))
    model.add(Conv2D(32, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))

    # initiate RMSprop optimizer
    opt = keras.optimizers.rmsprop(lr=lr, decay=decay)

    # Let's train the model using RMSprop
    model.compile(loss='categorical_crossentropy',
                  optimizer=opt,
                  metrics=['accuracy'])

    if not data_augmentation:
        print('Not using data augmentation.')
        model.fit(x_train, y_train,
                  batch_size=batch_size,
                  epochs=epochs,
                  validation_data=(x_test, y_test),
                  shuffle=True)
    else:
        print('Using real-time data augmentation.')
        # This will do preprocessing and realtime data augmentation:
        datagen = ImageDataGenerator(
            featurewise_center=False,  # set input mean to 0 over the dataset
            samplewise_center=False,  # set each sample mean to 0
            featurewise_std_normalization=False,  # divide inputs by std of the dataset
            samplewise_std_normalization=False,  # divide each input by its std
            zca_whitening=False,  # apply ZCA whitening
            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
            horizontal_flip=True,  # randomly flip images
            vertical_flip=False)  # randomly flip images

        # Compute quantities required for feature-wise normalization
        # (std, mean, and principal components if ZCA whitening is applied).
        datagen.fit(x_train)

        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(datagen.flow(x_train, y_train,
                                         batch_size=batch_size),
                            steps_per_epoch=x_train.shape[0] // batch_size,
                            epochs=epochs,
                            validation_data=(x_test, y_test),
                            workers=4)

    # Score trained model.
    scores = model.evaluate(x_test, y_test, verbose=1)
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])

    return model


def get_category(equivalence_classes, num_categories, label):
    '''
    Get the category of the equivalence classes
    '''
    category = label
    while category >= num_categories:
        category = equivalence_classes[label]
        label = category
    return category


def branch_out(label_train, correctcount_train, threshold=8):
    '''
    Relabel the misclassified samples (number of correct classification is less
    than the threshold) with new classes
    '''
    assert label_train.shape == correctcount_train.shape, "The shapes differ!"

    num_classes = np.max(label_train) + 1
    threshold = max(threshold, np.max(correctcount_train)/2)
    for i in range(len(label_train)):
        correctcount = correctcount_train[i]
        if correctcount < threshold:
            label_train[i] += num_classes
            
    return keras.utils.to_categorical(label_train, np.max(label_train)+1)


def binary_split(labels, correctcounts, x, threshold=1):
    assert labels.shape == correctcounts.shape, "The shapes differ!"
    
    num_classes = np.max(labels) + 1
    x_easy, x_hard = [], []
    labels_easy, labels_hard = [], []
    
    for i in range(len(labels)):
        correctcount = correctcounts[i]
        if correctcount < threshold:
            x_hard.append(x[i])
            labels_hard.append(labels[i])
        else:
            x_easy.append(x[i])
            labels_easy.append(labels[i])
            
    x_easy = np.asarray(x_easy)
    x_hard = np.asarray(x_hard)        
    y_easy = keras.utils.to_categorical(labels_easy, num_classes)
    y_hard = keras.utils.to_categorical(labels_hard, num_classes)
    
    return (x_easy, y_easy), (x_hard, y_hard)


def enumerate_splits(label_train, correctcount_train, x_train,  \
                     label_test, correctcount_test, x_test, max_threshold=10):
    ratios = np.zeros((max_threshold, max_threshold))
    for threshold_train in range(max_threshold):
        for threshold_test in range(max_threshold):
            (x_train_easy, y_train_easy), (x_train_hard, y_train_hard) = \
            binary_split(label_train, correctcount_train, x_train, threshold_train)
            (x_test_easy, y_test_easy), (x_test_hard, y_test_hard) = \
            binary_split(label_test, correctcount_test, x_test, threshold_test)
            ratios[threshold_train, threshold_test] = (len(x_train_easy)/len(x_train)) \
                                             / (len(x_test_easy)/len(x_test)) 
    return ratios


# TODO: implement "on-the-fly version of branchnet"
def resume_training(model, x_train, y_train, x_test, y_test,
                    batch_size=32, epochs=25, initial_epoch=1,
                    data_augmentation=True):
    """
    Resume training the model
    :param model:
    :param x_train: training data, size (N x h x w x 3)
    :param y_train: categorical ground truth for training data, size (N x C)
    :param x_test: testing data, size (N x h x w x 3)
    :param y_test: categorical ground truth for testing data, size (N x C)
    :param batch_size: batch size for training
    :param epochs: epochs for training
    :param initial_epoch: initial epoch for consistency
    :param data_augmentation: whether using data augmentation or not
    :return: the model after training
    """

    assert len(x_train) == len(y_train) and len(x_test) == len(y_test) and y_train.shape[1] == y_test.shape[1]
    num_classes = y_train.shape[1]

    if model.output_shape[1] != num_classes:
        model.pop()
        model.pop()
        model.add(Dense(num_classes, name='dense_-1'))
        model.add(Activation('softmax', name='activation_-1'))

    # No need to recompile this time
    if not data_augmentation:
        print('Not using data augmentation.')
        model.fit(x_train, y_train,
                  batch_size=batch_size,
                  epochs=epochs,
                  validation_data=(x_test, y_test),
                  shuffle=True)
    else:
        print('Using real-time data augmentation.')
        # This will do preprocessing and realtime data augmentation:
        datagen = ImageDataGenerator(
            featurewise_center=False,  # set input mean to 0 over the dataset
            samplewise_center=False,  # set each sample mean to 0
            featurewise_std_normalization=False,  # divide inputs by std of the dataset
            samplewise_std_normalization=False,  # divide each input by its std
            zca_whitening=False,  # apply ZCA whitening
            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
            horizontal_flip=True,  # randomly flip images
            vertical_flip=False)  # randomly flip images

        # Compute quantities required for feature-wise normalization
        # (std, mean, and principal components if ZCA whitening is applied).
        datagen.fit(x_train)

        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(datagen.flow(x_train, y_train,
                                         batch_size=batch_size),
                            steps_per_epoch=x_train.shape[0] // batch_size,
                            epochs=epochs,
                            validation_data=(x_test, y_test),
                            workers=4,
                            initial_epoch=initial_epoch)

    # Score trained model.
    scores = model.evaluate(x_test, y_test, verbose=1)
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])

    return model
